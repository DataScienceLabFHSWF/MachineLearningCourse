{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c718a5c1",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction with PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c516f7e",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca4fdbd",
   "metadata": {},
   "source": [
    "This notebook deals with the understanding and usage of the ___Principal Component Analysis (PCA)___ on different datasets. PCA is a model-based way to reduce the amount of \"uninformative\" or redundant information, meaning that it tries to reduce the amount of individual features while trying to lose as little information as possible.\n",
    "The newly obtained features from the PCA can then be used instead of the (higher-dimensional) original feature vector.\n",
    "\n",
    "__\"The key idea here is to replace redundant features with a few new features that adequately summarize information contained in the original feature space.\"__\n",
    "\n",
    "To gain some understanding we will first take a look at how to \"manually\" extract the principal components to then use tools provided by scikit-learn to achieve the same goal. Most of the information and content in _Task 1_ is taken from LINK and packed into this notebook to get a better overview as well as have all the information in one place."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65ec344",
   "metadata": {},
   "source": [
    "### Overview\n",
    "The main goal of __PCA__ is to create new features that maximize the variance in the data. This can be seen in the figure below, where the values for x1 and x2 are plotted on the x- and y-axis and the principal components vectors (eigenvectors of the covariance matrix) point into a different direction than the axes. Each principal component/eigenvector is perpendicular to each other, and the amount of contained information/variance decreases for every new component.\n",
    "\n",
    "<center>\n",
    "<img src=\"img/05_01.png\" width=\"400\" height=\"400\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518f3e49",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "#### Manual and Step-by-step\n",
    "First we are going to load, split and standardize an example dataset to perform the PCA on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fd429c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"wine.data\")\n",
    "\n",
    "df.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',\n",
    "                   'Alcalinity of ash', 'Magnesium', 'Total phenols',\n",
    "                   'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',\n",
    "                   'Color intensity', 'Hue',\n",
    "                   'OD280/OD315 of diluted wines', 'Proline']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20843347",
   "metadata": {},
   "source": [
    "Now we use sklearn train_test_split() to split our dataset into training and test data (80/20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd14105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = df.iloc[:,1:].values, df.iloc[:,0].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,stratify=y,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfcf540",
   "metadata": {},
   "source": [
    "Standardize data with sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af2d802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "X_train_std = std_scaler.fit_transform(X_train)\n",
    "X_test_std = std_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c869643",
   "metadata": {},
   "source": [
    "We can extract the principal components by performing an eigendecomposition on the covariance matrix of our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d039a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "cov_mat = np.cov(X_train_std.T)\n",
    "eigenvals, eigenvecs = np.linalg.eigh(cov_mat)\n",
    "\n",
    "print('\\nEigenvalues \\n%s' % eigenvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8adf66",
   "metadata": {},
   "source": [
    "Then we use these eigenvalues to calculate the amount of variance each of these components explains. Here we first calculate the total amount of variance by summing up all eigenvalues to then normalize each individual value by this sum. Additionally we track the cumulative explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47d19cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_total = sum(eigenvals)\n",
    "var_explained = [(i /var_total) for i in sorted(eigenvals, reverse=True)]\n",
    "cum_var_explained = np.cumsum(var_explained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a453d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "plt.bar(range(1, len(eigenvals)+1), var_explained, alpha=0.5, align='center',\n",
    "        label='Explained variance per component')\n",
    "plt.step(range(1, len(eigenvals)+1), cum_var_explained, where='mid',\n",
    "         label='Cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal component')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33cfa78",
   "metadata": {},
   "source": [
    "##### Feature transformation\n",
    "In order to use these newly aquired principal components as features we need to perform a base transformation where we use the PCs as new axes in our coordinate system. This will lead to unrecognizable data as the values of the original features are transformed. \n",
    "Below we first combine eigenvalues and eigenvectors into a list of tuples (eigen_pairs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ea0d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "# eigenvectors stored as columns of a matrix -> [:, i]\n",
    "eigen_pairs = [(np.abs(eigenvals[i]), eigenvecs[:, i])\n",
    "               for i in range(len(eigenvals))]\n",
    "\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "eigen_pairs.sort(key=lambda k: k[0], reverse=True)\n",
    "\n",
    "eigen_pairs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989806b9",
   "metadata": {},
   "source": [
    "This list of eigen_pairs has ofc. 13 elements, one for each principal component. Each of the elements of this list are tuples, and each tuple contains the eigenvalue as well as the eigenvector coordinates for each dimension. As PCA is often used for dimensionality reduction lets assume we want only to use the first two principal components (the ones with the highest explained variance as we want to capture as much underlying information as possible).\n",
    "\n",
    "__Exercise:__ How much of the original variance was captured by the first two principal components? How much additional explained variance is provided by the third PC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aac608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "print(\"Variance explained by 1. PC: {}  2. PC: {}  cum.: {}\".format(var_explained[0],var_explained[1],cum_var_explained[1]))\n",
    "print(\"Variance explained by 1. PC: {}  2. PC: {}  3. PC: {}  cum.: {}\".format(var_explained[0],var_explained[1],var_explained[2],cum_var_explained[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe179d2",
   "metadata": {},
   "source": [
    "As mentioned above we only want to use the first two PCs of the 13 that were derived. These two components capture more than half of the explained variance of the underlying original dataset and could thus be a suitable choice to perform dimension reduction while simultaneously minimizing the loss of information. We use _numpy.hstack_ to create a new matrix with two columns that contains all coordinate values for the first two eigenvectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7799a79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.hstack((eigen_pairs[0][1][:, np.newaxis],\n",
    "               eigen_pairs[1][1][:, np.newaxis]))\n",
    "print('Matrix W:\\n', w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557f7e7e",
   "metadata": {},
   "source": [
    "__Exercise:__ Use the code from above to create a matrix _w2_ from the coordinates of the four most informative components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21997ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2 = np.hstack((eigen_pairs[0][1][:, np.newaxis],\n",
    "               eigen_pairs[1][1][:, np.newaxis],\n",
    "               eigen_pairs[2][1][:, np.newaxis],\n",
    "               eigen_pairs[3][1][:, np.newaxis]))\n",
    "print('Matrix W:\\n', w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b984720",
   "metadata": {},
   "source": [
    "Now that we have our matrix _w_ we can transform our data by taking the dot product of the original data and the PCA-column vectors, projecting the original data onto the PCA axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8249f8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train_std[0]: {}\\n\".format(X_train_std[0]))\n",
    "print(\"X_train_std[0].dot(w): {}\".format(X_train_std[0].dot(w)))\n",
    "\n",
    "X_train_pca = X_train_std.dot(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837a0873",
   "metadata": {},
   "source": [
    "When plotting the results we can see that the first two PCs already achieve quite good clustering, even though they "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8ce06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['r', 'b', 'g']\n",
    "markers = ['s', 'x', 'o']\n",
    "\n",
    "for l, c, m in zip(np.unique(y_train), colors, markers):\n",
    "    plt.scatter(X_train_pca[y_train == l, 0], \n",
    "                X_train_pca[y_train == l, 1], \n",
    "                c=c, label=l, marker=m)\n",
    "\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5fbb08",
   "metadata": {},
   "source": [
    "__Exercise:__ Now take only the best and the worst PCs and use them as basis for the feature transformation. How does the resulting scatter plot differ from the one shown above? What's the reason behind this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868b534e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation here\n",
    "w3 = np.hstack((eigen_pairs[0][1][:, np.newaxis],\n",
    "               eigen_pairs[-1][1][:, np.newaxis]))\n",
    "\n",
    "X_train_pca_last = X_train_std.dot(w3)\n",
    "\n",
    "colors = ['r', 'b', 'g']\n",
    "markers = ['s', 'x', 'o']\n",
    "\n",
    "for l, c, m in zip(np.unique(y_train), colors, markers):\n",
    "    plt.scatter(X_train_pca_last[y_train == l, 0], \n",
    "                X_train_pca_last[y_train == l, 1], \n",
    "                c=c, label=l, marker=m)\n",
    "\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 13')\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f586a8c",
   "metadata": {},
   "source": [
    "### PCA with scikit-learn\n",
    "Of course you do not always have to perform these tasks by hand as there is already a scikit-learn implementation for the PCA. This makes it more comfortable to use and takes care of the underlying math. To perform the same operations as above we just have to do the following when utilizing sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba329afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c00de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.bar(range(1, len(pca.explained_variance_ratio_)+1), pca.explained_variance_ratio_, alpha=0.5, align='center',\n",
    "        label='Explained variance per component')\n",
    "plt.step(range(1, len(pca.explained_variance_ratio_)+1), np.cumsum(pca.explained_variance_ratio_), where='mid',\n",
    "         label='Cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal component')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bd4967",
   "metadata": {},
   "source": [
    "## Task 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfb8c84",
   "metadata": {},
   "source": [
    "In this task you will take a closer look at the _Iris_ dataset (\"IRIS.csv\", __[download from here](https://github.com/DataScienceLabFHSWF/machine-learning-book/tree/main/data/pca)__) which lists different attributes of the Iris flower, taken from various samples. The goal here is to predict the correct species by applying and comparing different machine learning algorithms of your choice.  \n",
    "Perform PCA on the data and take a look at the _explained variance_ in order to get the best trade-off between dimensionality-reduction and information loss. Train and test every model you consider on the PCA-transformed and the original data to see the impact of the aforementioned loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7248c2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"IRIS.csv\")\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6fa8a5",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "Plot the distribution of the different features in the dataset. What kind of scaling would you choose for the different attributes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cb4b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sepal_length\"].hist(bins=10)\n",
    "plt.title(\"Distribution for sepal_length attribute of IRIS dataset\")\n",
    "plt.xlabel(\"sepal length in cm\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.show()\n",
    "\n",
    "df[\"sepal_width\"].hist(bins=10)\n",
    "plt.title(\"Distribution for sepal_width attribute of IRIS dataset\")\n",
    "plt.xlabel(\"sepal width in cm\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.show()\n",
    "\n",
    "df[\"petal_length\"].hist(bins=10)\n",
    "plt.title(\"Distribution for petal_length attribute of IRIS dataset\")\n",
    "plt.xlabel(\"petal length in cm\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.show()\n",
    "\n",
    "df[\"petal_width\"].hist(bins=10)\n",
    "plt.title(\"Distribution for petal_width attribute of IRIS dataset\")\n",
    "plt.xlabel(\"petal width in cm\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d93a40",
   "metadata": {},
   "source": [
    "Use the _seaborn_ package to plot a heatmap of the correlation matrix of the dataframe (df.corr()). Which attributes are highly correlated, which feature sticks out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10584fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr(),cmap=sns.color_palette(\"Spectral\", as_cmap=True))\n",
    "plt.title(\"Correlation matrix for IRIS flower dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe82e27",
   "metadata": {},
   "source": [
    "### Preprocess\n",
    "Preprocess the data here (label encoding, scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f381ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "mm_scaler = preprocessing.MinMaxScaler()\n",
    "std_scaler = preprocessing.StandardScaler()\n",
    "\n",
    "df[\"species\"] = le.fit_transform(df[\"species\"])\n",
    "df[\"sepal_length\"] = std_scaler.fit_transform(df[[\"sepal_length\"]])\n",
    "df[\"sepal_width\"] = std_scaler.fit_transform(df[[\"sepal_width\"]])\n",
    "df[\"petal_length\"] = mm_scaler.fit_transform(df[[\"petal_length\"]])\n",
    "df[\"petal_width\"] = mm_scaler.fit_transform(df[[\"petal_width\"]])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b517e6db",
   "metadata": {},
   "source": [
    "### PCA\n",
    "### Apply and create dataframe\n",
    "Split the dataframe into two separate dataframes (feature data and target data (X,y)). Apply PCA to the feature data, build a new dataframe from the transformed components and store it as _X\\_pca_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b84bb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X = df.drop(\"species\",axis=1)\n",
    "y = df[\"species\"]\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "X_pca = pca.fit_transform(X)\n",
    "# X_train_pca = pca.fit_transform(X_train_std)\n",
    "\n",
    "\n",
    "# plt.bar(range(1, len(pca.explained_variance_ratio_)+1), pca.explained_variance_ratio_, alpha=0.5, align='center',\n",
    "#         label='Explained variance per component')\n",
    "# plt.step(range(1, len(pca.explained_variance_ratio_)+1), np.cumsum(pca.explained_variance_ratio_), where='mid',\n",
    "#          label='Cumulative explained variance')\n",
    "# plt.ylabel('Explained variance ratio')\n",
    "# plt.xlabel('Principal component')\n",
    "# plt.legend(loc='best')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362cbc52",
   "metadata": {},
   "source": [
    "Plot the heatmap of the PCA-transformed dataframe and compare it to the map from above. How can you explain the differences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f9ee28",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(pd.DataFrame(X_pca).corr(),cmap=sns.color_palette(\"Spectral\", as_cmap=True))\n",
    "plt.title(\"Correlation matrix for IRIS flower dataset (after PCA)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab26b9bb",
   "metadata": {},
   "source": [
    "Take a look at the explained variance of the principal components and display display it in a plot. How many components would you choose to get the best trade-off between dimensionality-reduction and information loss? How much of the variance in the entire dataset do they represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdbf8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_var = pca.explained_variance_ratio_\n",
    "print(\"explained variance (%): {}\".format(np.array(explained_var)*100))\n",
    "\n",
    "labels = [\"PC{}\".format(int(i)) for i in range(1,len(explained_var)+1)]\n",
    "\n",
    "print(labels)\n",
    "plt.bar(labels, explained_var*100, alpha=0.5, align='center', label='Explained variance per component')\n",
    "plt.title(\"Explained variance per principal component\")\n",
    "plt.xlabel(\"Principal component\")\n",
    "plt.ylabel(\"Explained Variance (in %)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab01d5f8",
   "metadata": {},
   "source": [
    "Now create a dataframe out of your chosen components (_np.hstack()_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a482976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pca = pd.DataFrame(X_pca)\n",
    "\n",
    "ft_pcs = np.hstack((X_pca[:,0][:, np.newaxis],\n",
    "                    X_pca[:,1][:, np.newaxis]))\n",
    "                    \n",
    "X2 = pd.DataFrame(ft_pcs, columns=labels[:2])\n",
    "X2 = pd.concat([X2,pd.DataFrame(y)],axis=1)\n",
    "X2\n",
    "\n",
    "### Alternative\n",
    "# pca2 = PCA(2)\n",
    "# X_pca2 = pca2.fit_transform(X)\n",
    "# X2 = pd.DataFrame(X_pca22)\n",
    "# X2 = pd.concat([X2,pd.DataFrame(y)],axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ed48f2",
   "metadata": {},
   "source": [
    "Create a scatter plot of the first two PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6078ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['r', 'b', 'g']\n",
    "markers = ['s', 'x', 'o']\n",
    "\n",
    "for l, c, m in zip(np.unique(y), colors, markers):\n",
    "    plt.scatter(X2[y == l][\"PC1\"], \n",
    "                X2[y == l][\"PC2\"],  \n",
    "                c=c, label=l, marker=m)\n",
    "\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa14c3b",
   "metadata": {},
   "source": [
    "### Training\n",
    "Split the datasets (original and PCA-transformed) into train and test data and use different algorithms of your choice to try and achieve the highest target score.\n",
    "Use _train\\_test\\_splot from _sklearn.model_selection_ (random_state: 25, test_size: 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27cb5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=25,test_size=0.25)\n",
    "X_pca_train, X_pca_test, y_train, y_test = train_test_split(X2,y,random_state=25,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ccc194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "algorithms = [LogisticRegression(),GaussianNB(),RandomForestClassifier(),KNeighborsClassifier(3)]\n",
    "algorithm_names = [\"LogisticRegression\",\"GaussianNB\",\"RandomForest\",\"KNeighborsClassifier\"]\n",
    "\n",
    "models = []\n",
    "scores = []\n",
    "\n",
    "print(\"Without PCA:\")\n",
    "for i,algo in enumerate(algorithms):\n",
    "    model = algo.fit(X_train,y_train)\n",
    "    score = algo.score(X_test,y_test)\n",
    "    models.append(model)\n",
    "    scores.append(score)\n",
    "    print(f\"{algorithm_names[i]}: {score}\")\n",
    "\n",
    "print(f\"\\nWith PCA:\")\n",
    "for i,algo in enumerate(algorithms):\n",
    "    model = algo.fit(X_pca_train,y_train)\n",
    "    score = algo.score(X_pca_test,y_test)\n",
    "    models.append(model)\n",
    "    scores.append(score)\n",
    "    print(f\"{algorithm_names[i]}: {score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c49c0c4",
   "metadata": {},
   "source": [
    "Plot the results. Which of the algorithms you chose performed best? Do the results surprise you in any way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471e8332",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(algorithm_names, scores[:4], alpha=0.5, align='center', label='non-PCA')\n",
    "plt.bar(algorithm_names, scores[4:8], alpha=0.5, align='center', label='PCA')\n",
    "plt.title(\"Prediction score for different algorithms\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8104c95c",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "In this task we try to predict the anticipated final status of bank loans, given various attributes describing the loan taker. The _credit_ dataset for this task can be downloaded from __[here](https://github.com/DataScienceLabFHSWF/machine-learning-book/tree/main/data/pca)__.  \n",
    "As always, load the data into a dataframe and get familiar with it (__[might be useful](https://www.investopedia.com/terms/c/chargeoff.asp)__). What kinds of categories do you see (numerical, categorical)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962842ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "df = pd.read_csv(\"credit.csv\")\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4821f7",
   "metadata": {},
   "source": [
    "Are there any features that correlate strongly with each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931bbc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr(),cmap=sns.color_palette(\"Spectral\", as_cmap=True))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baf8b14",
   "metadata": {},
   "source": [
    "### Dealing with missing data\n",
    "You might have noticed that there are quite a few missing/na values in our dataset. Take a closer look at each of the attributes that has missing values and try to decide how to best deal with it (fill with constant value, drop entirely, ...).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec490ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(df.isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77574568",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Credit Score\"].describe()\n",
    "df[\"Credit Score\"].info()\n",
    "df[\"Credit Score\"] = df[\"Credit Score\"].fillna(df[\"Credit Score\"].median())\n",
    "df[\"Credit Score\"].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e1c9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Annual Income\"] = df[\"Annual Income\"].fillna(df[\"Annual Income\"].median())\n",
    "np.sum(df.isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a201725",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.Bankruptcies.value_counts())\n",
    "df.Bankruptcies.fillna(0.0,inplace=True)\n",
    "\n",
    "np.sum(df.isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e235ee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"Maximum Open Credit\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea2880f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"Maximum Open Credit\",\"Years in current job\",\"Tax Liens\"])\n",
    "np.sum(df.isna())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a669979e",
   "metadata": {},
   "source": [
    "### Categorical variables\n",
    "At this point your dataframe should have no attributes with missing values left. In the next step you should use One-Hot-Encoding or Label-Encoding to transform the categorical values into numerical/binary ones.  \n",
    "What are the names of these features? Which technique(s) did you use?  \n",
    "\n",
    "First encode the target column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8162a6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_enc = preprocessing.LabelEncoder()\n",
    "\n",
    "print(df[\"Loan Status\"].value_counts())\n",
    "df[\"Loan Status\"] = label_enc.fit_transform(df[\"Loan Status\"])\n",
    "print(df[\"Loan Status\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6f284c",
   "metadata": {},
   "source": [
    "And now the other attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4c6e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Years in current job\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851118ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df.dropna(subset=[\"Years in current job\"]).drop([\"Months since last delinquent\"],axis=1)\n",
    "np.sum(df_cleaned.isna())\n",
    "len(df_cleaned.columns)\n",
    "df_cleaned.info()\n",
    "df_cleaned.describe()\n",
    "len(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413acf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.dtypes\n",
    "columns_purpose = np.unique(df_cleaned[\"Purpose\"].values)\n",
    "columns_ho = np.unique(df_cleaned[\"Home Ownership\"].values)\n",
    "columns_term = np.unique(df[\"Term\"].values)\n",
    "columns_yicj = np.unique(df[\"Years in current job\"].values)\n",
    "\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "bin_purpose = lb.fit_transform(df.Purpose)\n",
    "bin_home_owner = lb.fit_transform(df[\"Home Ownership\"])\n",
    "bin_term = le.fit_transform(df[\"Term\"])\n",
    "bin_years_in_current_job = lb.fit_transform(df[\"Years in current job\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6860ac66",
   "metadata": {},
   "source": [
    "Now append these transformed matrizes to your cleaned dataframe (_pd.concat()_) to create the final version of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d71853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dic = {\"HO_0\":f1[:,0],\"HO_1\":f1[:,1],\"HO_2\":f1[:,2],\"HO_3\":f1[:,3]}\n",
    "# df_cleaned[\"HO_0\"] = f1[:,0]\n",
    "# df_cleaned.append(dic,ignore_index=True)\n",
    "# df_cleaned.append(f1.tolist())\n",
    "df_cleaned_new = pd.concat([df_cleaned, pd.DataFrame(bin_purpose, columns=columns_purpose).set_index(df_cleaned.index)], axis=1)\n",
    "df_cleaned_new = pd.concat([df_cleaned_new, pd.DataFrame(bin_home_owner, columns=columns_ho).set_index(df_cleaned_new.index)], axis=1)\n",
    "df_cleaned_new = pd.concat([df_cleaned_new, pd.DataFrame(bin_years_in_current_job, columns=columns_yicj).set_index(df_cleaned_new.index)], axis=1)\n",
    "df_cleaned_new[\"Term\"] = bin_term\n",
    "\n",
    "# df_cleaned_new.drop([\"Purpose\",\"Home Ownership\",\"Years in current job\"],axis=1,inplace=True)\n",
    "df_cleaned_new.drop([\"Years of Credit History\",\"Number of Credit Problems\",\"Number of Open Accounts\",\"Purpose\",\"Home Ownership\",\"Years in current job\",\"Term\",\"Credit Score\",\"Loan Status\",\"Customer ID\",\"Loan ID\",\"Annual Income\",\"Monthly Debt\"],axis=1,inplace=True)\n",
    "df_cleaned_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8934ef03",
   "metadata": {},
   "source": [
    "### PCA\n",
    "Perform the PCA and plot the important metrics. How many components are needed to explain most of the variance?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c40647",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_cleaned_new.iloc[:,4:]\n",
    "y = df_cleaned_new.iloc[:,3]\n",
    "\n",
    "\n",
    "pca_model = PCA()\n",
    "X_pca = pca_model.fit_transform(X)\n",
    "\n",
    "plt.bar(range(1, len(pca_model.explained_variance_ratio_)+1), pca_model.explained_variance_ratio_, alpha=0.5, align='center',\n",
    "        label='Explained variance per component')\n",
    "# plt.step(range(1, len(pca.explained_variance_ratio_)+1), np.cumsum(pca.explained_variance_ratio_), where='mid',\n",
    "#          label='Cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal component')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eb7c1b",
   "metadata": {},
   "source": [
    "### Training\n",
    "Use the cleaned and encoded dataframe for training and test data. Compare the results of PCA-transformed and the cleaned, untransformed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25d039d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_cleaned[df_cleaned[\"Loan Status\"]==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bc01d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_model1 = PCA(1)\n",
    "pca_model2 = PCA(2)\n",
    "X_pca1 = pca_model1.fit_transform(X)\n",
    "X_pca2 = pca_model2.fit_transform(X)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42,test_size=0.25) \n",
    "\n",
    "X_pca1_train, X_pca1_test, y_train, y_test = train_test_split(X_pca1,y,random_state=42,test_size=0.25) \n",
    "X_pca2_train, X_pca2_test, y_train, y_test = train_test_split(X_pca2,y,random_state=42,test_size=0.25) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64b20cc",
   "metadata": {},
   "source": [
    "### Evaluation and results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ef63b8",
   "metadata": {},
   "source": [
    "Apply various machine learning algorithms to your training data and measure and plot the results. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105efa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "algorithms = [LogisticRegression(),RandomForestClassifier(),KNeighborsClassifier(3)]\n",
    "datasets = [[X_train, X_test, y_train, y_test],[X_pca1_train, X_pca1_test, y_train, y_test],[X_pca2_train, X_pca2_test, y_train, y_test]]\n",
    "\n",
    "models = []\n",
    "scores = []\n",
    "\n",
    "for i,ds in enumerate(datasets):\n",
    "    print(f\"Dataset {i}:\")\n",
    "    for algo in algorithms:\n",
    "        model = algo.fit(ds[0],ds[2])\n",
    "        score = algo.score(ds[1],ds[3])\n",
    "        models.append(model)\n",
    "        scores.append(score)\n",
    "        print(f\"{str(algo)}: {score}\")\n",
    "        \n",
    "# print(\"Without PCA:\")\n",
    "# for i,algo in enumerate(algorithms):\n",
    "#     model = algo.fit(X_train,y_train)\n",
    "#     score = algo.score(X_test,y_test)\n",
    "#     models.append(model)\n",
    "#     scores.append(score)\n",
    "#     print(f\"{str(algo)}: {score}\")\n",
    "\n",
    "# print(f\"\\nWith PCA(1):\")\n",
    "# for i,algo in enumerate(algorithms):\n",
    "#     model = algo.fit(X_pca1_train,y_train)\n",
    "#     score = algo.score(X_pca1_test,y_test)\n",
    "#     models.append(model)\n",
    "#     scores.append(score)\n",
    "#     print(f\"{str(algo)}: {score}\")\n",
    "\n",
    "# print(f\"\\nWith PCA(2):\")\n",
    "# for i,algo in enumerate(algorithms):\n",
    "#     model = algo.fit(X_pca2_train,y_train)\n",
    "#     score = algo.score(X_pca2_test,y_test)\n",
    "#     models.append(model)\n",
    "#     scores.append(score)\n",
    "#     print(f\"{str(algo)}: {score}\")\n",
    "\n",
    "knn_testrange = range(1,20)\n",
    "knn_scores = []\n",
    "\n",
    "for i in knn_testrange:\n",
    "    model = KNeighborsClassifier(i).fit(X_train,y_train)\n",
    "    knn_scores.append(model.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03a06ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(knn_testrange,knn_scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eba7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_names = [str(el) for el in algorithms]\n",
    "\n",
    "plt.bar(algorithm_names, scores[:3], alpha=0.5, align='center', label='non-PCA')\n",
    "plt.bar(algorithm_names, scores[3:6], alpha=0.5, align='center', label='PCA(1)')\n",
    "plt.bar(algorithm_names, scores[6:9], alpha=0.5, align='center', label='PCA(2)')\n",
    "plt.title(\"Prediction score for different algorithms\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('airflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "568ce6a90abe48cd4c71813e2e18d608b5934a77a079188d46a97b4cb4032653"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
