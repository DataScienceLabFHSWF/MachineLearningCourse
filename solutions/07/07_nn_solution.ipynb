{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "In this task you have to implement a neural network from scratch. And just like in the exercise before, where you had to implement a perceptron, you again have to tackle this task without the utilization of any frameworks like TensorFlow or PyTorch.  \n",
    "The goal of this task is to verify your manually calculated results, thus you should print out the necessary information during the training process of the network. Besides that your class should implement the following methods:\n",
    "- init()\n",
    "- fit()\n",
    "- forward()/predict()\n",
    "- backward()\n",
    "- evaluate()\n",
    "\n",
    "Additionally your implementation should display the epoch number, accuracy and further parameters of your choice during the training process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation\n",
    "Imports here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement your class representing the neural network and all of its previously mentioned methods here:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##########################\n",
    "### MODEL\n",
    "##########################\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1. / (1. + np.exp(-z))\n",
    "\n",
    "\n",
    "def int_to_onehot(y, num_labels):\n",
    "\n",
    "    ary = np.zeros((y.shape[0], num_labels))\n",
    "    for i, val in enumerate(y):\n",
    "        ary[i, val] = 1\n",
    "\n",
    "    return ary\n",
    "\n",
    "\n",
    "class NeuralNetMLP:\n",
    "\n",
    "    def __init__(self, num_features, num_hidden, num_classes, learning_rate=0.1, random_seed=123):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # hidden\n",
    "        # rng = np.random.RandomState(random_seed)\n",
    "\n",
    "        # self.weight_h = rng.normal(\n",
    "        #     loc=0.0, scale=0.1, size=(num_hidden, num_features))\n",
    "        # self.weight_h = np.array([[0.3,0.1],[0.7,0.5]])\n",
    "        self.weight_h = np.array([[0.3,0.7],[-0.1,0.5]])\n",
    "        self.bias_h = np.zeros(num_hidden)\n",
    "        self.bias_h = np.array([0.4,0.4])\n",
    "        # # output\n",
    "        # self.weight_out = rng.normal(\n",
    "        #     loc=0.0, scale=0.1, size=(num_classes, num_hidden))\n",
    "        # self.bias_out = np.zeros(num_classes)\n",
    "        self.weight_out = np.array([[0.6,-0.7],[0.3,0.8]])\n",
    "        self.bias_out = np.array([0.15,0.15])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Hidden layer\n",
    "        # input dim: [n_examples, n_features] dot [n_hidden, n_features].T\n",
    "        # output dim: [n_examples, n_hidden]\n",
    "        z_h = np.dot(x, self.weight_h.T) + self.bias_h\n",
    "        a_h = sigmoid(z_h)\n",
    "        print(f\"z_h: {z_h}  a_h: {a_h}\")\n",
    "        # Output layer\n",
    "        # input dim: [n_examples, n_hidden] dot [n_classes, n_hidden].T\n",
    "        # output dim: [n_examples, n_classes]\n",
    "        z_out = np.dot(a_h, self.weight_out.T) + self.bias_out\n",
    "        a_out = sigmoid(z_out)\n",
    "        print(f\"z_out: {z_out}  a_out: {a_out}\")\n",
    "        return a_h, a_out\n",
    "\n",
    "    def backward(self, x, a_h, a_out, y):  \n",
    "    \n",
    "        #########################\n",
    "        ### Output layer weights\n",
    "        #########################\n",
    "        \n",
    "        # onehot encoding\n",
    "        # y_onehot = int_to_onehot(y, self.num_classes)\n",
    "        y_onehot = np.array(y)\n",
    "        # Part 1: dLoss/dOutWeights\n",
    "        ## = dLoss/dOutAct * dOutAct/dOutNet * dOutNet/dOutWeight\n",
    "        ## where DeltaOut = dLoss/dOutAct * dOutAct/dOutNet\n",
    "        ## for convenient re-use\n",
    "        \n",
    "        # input/output dim: [n_examples, n_classes]\n",
    "        d_loss__d_a_out = 2.*(a_out - y_onehot) / y.shape[0]\n",
    "        d_loss__d_a_out = (a_out - y_onehot)\n",
    "        print(f\"d_loss__d_a_out: {d_loss__d_a_out}\")\n",
    "        print(y_onehot)\n",
    "        # input/output dim: [n_examples, n_classes]\n",
    "        d_a_out__d_z_out = a_out * (1. - a_out) # sigmoid derivative\n",
    "        print(f\"d_a_out__d_z_out: {d_a_out__d_z_out}\")\n",
    "        # output dim: [n_examples, n_classes]\n",
    "        delta_out = d_loss__d_a_out * d_a_out__d_z_out # \"delta (rule) placeholder\"\n",
    "        # print(f\"delta_out: {delta_out}\")\n",
    "        # gradient for output weights\n",
    "        \n",
    "        # [n_examples, n_hidden]\n",
    "        d_z_out__dw_out = a_h\n",
    "        \n",
    "        # input dim: [n_classes, n_examples] dot [n_examples, n_hidden]\n",
    "        # output dim: [n_classes, n_hidden]\n",
    "        print(f\"delta_out.T: {delta_out.T}  d_z_out__dw_out: {d_z_out__dw_out}\")\n",
    "        d_loss__dw_out = np.dot(delta_out.T, d_z_out__dw_out)\n",
    "        print(f\"d_loss__dw_out: {d_loss__dw_out}\")\n",
    "        d_loss__db_out = np.sum(delta_out, axis=0)\n",
    "        print(f\"d_loss__db_out: {d_loss__db_out}\")\n",
    "        \n",
    "\n",
    "        #################################        \n",
    "        # Part 2: dLoss/dHiddenWeights\n",
    "        ## = DeltaOut * dOutNet/dHiddenAct * dHiddenAct/dHiddenNet * dHiddenNet/dWeight\n",
    "        \n",
    "        # [n_classes, n_hidden]\n",
    "        d_z_out__a_h = self.weight_out\n",
    "        \n",
    "        # output dim: [n_examples, n_hidden]\n",
    "        d_loss__a_h = np.dot(delta_out, d_z_out__a_h)\n",
    "        \n",
    "        # [n_examples, n_hidden]\n",
    "        d_a_h__d_z_h = a_h * (1. - a_h) # sigmoid derivative\n",
    "        \n",
    "        # [n_examples, n_features]\n",
    "        d_z_h__d_w_h = x\n",
    "        \n",
    "        # output dim: [n_hidden, n_features]\n",
    "        d_loss__d_w_h = np.dot((d_loss__a_h * d_a_h__d_z_h).T, d_z_h__d_w_h)\n",
    "        d_loss__d_b_h = np.sum((d_loss__a_h * d_a_h__d_z_h), axis=0)\n",
    "\n",
    "        return d_loss__dw_out, d_loss__db_out, d_loss__d_w_h, d_loss__d_b_h\n",
    "\n",
    "    def get_accuracy(self, y_correct, y_preds):\n",
    "        return np.mean(y_correct == y_preds)\n",
    "    \n",
    "    def train(self,x,y,epochs):\n",
    "        y = np.array(y)\n",
    "        for epoch in range(epochs):\n",
    "            ah, aout = self.forward(x)\n",
    "            #print(f\"ah: {ah}   aout: {aout}\")\n",
    "            d_loss__dw_out, d_loss__db_out, d_loss__d_w_h, d_loss__d_b_h = self.backward(x,ah,aout,y)\n",
    "            #print(f\"d_loss__dw_out: {d_loss__dw_out} d_loss__db_out: {d_loss__db_out} \")\n",
    "            self.weight_h -= self.learning_rate * d_loss__d_w_h\n",
    "            self.bias_h -= self.learning_rate * d_loss__d_b_h\n",
    "            self.weight_out -= self.learning_rate * d_loss__dw_out\n",
    "            self.bias_out -= self.learning_rate * d_loss__db_out\n",
    "\n",
    "            print(f\"self.weight_h: {self.weight_h}   self.weight_out: {self.weight_out}  acc:{self.get_accuracy(y,aout)} ah: {ah}   aout: {aout}\")\n",
    "\n",
    "            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define input and target lists here. Create an instance of your network class and use it to perform at least one training step. Does the output match with your calculations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [0.2,0.6]\n",
    "y = [1,0]\n",
    "\n",
    "nn = NeuralNetMLP(2,2,2,0.1)\n",
    "\n",
    "ah, aout = nn.forward(x)\n",
    "nn.backward(x,ah,aout,np.array(y))\n",
    "nn.train(x,y,1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('airflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "568ce6a90abe48cd4c71813e2e18d608b5934a77a079188d46a97b4cb4032653"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
