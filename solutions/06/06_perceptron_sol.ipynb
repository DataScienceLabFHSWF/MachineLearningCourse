{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "This task is split into two parts, first you have to implement a _perceptron_ that is capable of handling two input neurons and use it to verify your manually calculated results from the previous task.  \n",
    "In _b)_ your implementation has to be modified so that it allows for the perceptron to deal with an arbitrary amount of inputs.  \n",
    "You do not necessarily have to implement the simpler version first, you can also directly skip to part _b)_ and run the verifications from _a)_ on the more complex model if you like to.\n",
    "### a)\n",
    "In this task you have to implement a _perceptron_ from scratch without using any frameworks like tensorflow or pytorch but you can use packages like _numpy_, _sklearn_ etc. Your implementation should provide the following methods:\n",
    "- init()\n",
    "- fit()\n",
    "- predict()\n",
    "- evaluate()\n",
    "\n",
    "Make sure to output all the relevant values during each step to be able to compare it to your own calculations.\n",
    "Additionally it should print out the epoch number, accuracy and further parameters of your choice after each training epoch.\n",
    " \n",
    "  \n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation as class\n",
    "Imports here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import itertools\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement your class representing the perceptron and all of its previously mentioned methods here:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, w1, w2, bias):\n",
    "        self.bias = bias\n",
    "        self.w1 = w1\n",
    "        self.w2 = w2\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.array([1 if ((self.w1*x[0] + self.w2*x[1] + self.bias) >= 0) else 0 for x in X])\n",
    "        \n",
    "\n",
    "    def evaluate(self,y_correct,y_predict):\n",
    "        return sum(np.array(y_correct == y_predict).astype(int))/len(y_correct)\n",
    "    \n",
    "    def fit(self,X,ys,epochs=100,learning_rate=0.1):\n",
    "        for epoch in range(epochs):\n",
    "            for el in zip(X,ys):\n",
    "                x1 = el[0][0]\n",
    "                x2 = el[0][1]\n",
    "                y = el[1]\n",
    "                error = y-self.predict(np.array([el[0]]))\n",
    "\n",
    "                self.w1 += learning_rate * error * x1\n",
    "                self.w2 += learning_rate * error * x2\n",
    "                self.bias += learning_rate * error\n",
    "                \n",
    "                print(f\"w1: {self.w1} w2: {self.w2} bias: {self.bias} error: {error}\")\n",
    "\n",
    "            print(f\"epoch: {epoch+1}  acc: {self.evaluate(ys,self.predict(X))} w1: {self.w1}  w2: {self.w2}  bias: {self.bias}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test your perceptron with the same initial values as used for your manual calculations, once for AND and XOR each. Do you get the same results? \n",
    "Play around with the number of epochs. How many do you need in both of the models to reach an accuracy of 1?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AND:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pct = Perceptron(1.54,-3.4,0.2)\n",
    "pct = Perceptron(0.6,1,0.1)\n",
    "# pct = Perceptron(0.2,0.6,0.2)\n",
    "data = np.array([[0,0],[1,0],[0,1],[1,1]])\n",
    "labels = np.array([0,0,0,1])\n",
    "\n",
    "pct.fit(data,labels,epochs=5)\n",
    "pct.predict(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XOR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct = Perceptron(-0.5,0.6,0.2)\n",
    "\n",
    "data = np.array([[0,0],[1,0],[0,1],[1,1]])\n",
    "labels = np.array([0,1,1,0])\n",
    "\n",
    "pct.fit(data,labels,epochs=5)\n",
    "pct.predict(data)\n",
    "\n",
    "plt.scatterplot(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)\n",
    "In this subtask you first have to implement an improved version of your _perceptron_ from _a)_ and then generate data to test its functionality. At the end you will use the data to compare the impact of different learning rates on convergence and plot series.  \n",
    "In the following code block you should implement your improved _perceptron_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptronArbitrary:\n",
    "    def __init__(self, w, bias):\n",
    "        self.bias = bias\n",
    "        self.w = w\n",
    "        self.acc = []\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.array([1 if ((np.dot(self.w,x) + self.bias) >= 0) else 0 for x in X])\n",
    "\n",
    "    def evaluate(self,y_correct,y_predict):\n",
    "        return sum(np.array(y_correct == y_predict).astype(int))/len(y_correct)\n",
    "    \n",
    "    def fit(self,X,ys,epochs=100,learning_rate=0.1):\n",
    "        for epoch in range(epochs):\n",
    "            for el in zip(X,ys):\n",
    "                x = el[0]\n",
    "                y = el[1]\n",
    "                error = y-self.predict(np.array([x]))\n",
    "\n",
    "                self.bias += learning_rate * error\n",
    "                self.w += learning_rate * error * x\n",
    "\n",
    "                print(f\"w: {self.w} bias: {self.bias} error: {error}\")\n",
    "\n",
    "            acc = self.evaluate(ys,self.predict(X))\n",
    "            self.acc.append(acc)\n",
    "            print(f\"epoch: {epoch}  acc: {acc} w: {self.w} bias: {self.bias}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement tests here by utilizing a package called _itertools_ to generate _AND_, _OR_ or _XOR_ tables with arbitrary amounts of bits and then use these to feed them into the modified perceptron. An example of its usage for this particular case is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[int(p) for p in perm] for perm in itertools.product(\"01\", repeat=3)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try it out for yourself and test if your model can handle an arbitrary number of inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "np.random.seed(42)\n",
    "\n",
    "data = [[int(s) for s in seq] for seq in itertools.product(\"01\", repeat=5)]\n",
    "labels = [0]*(len(data)-1)+[1]\n",
    "\n",
    "pct = PerceptronArbitrary(np.random.randn(int(math.sqrt(len(data)))),np.random.randn())\n",
    "\n",
    "pct.fit(data,labels,epochs=epochs,learning_rate=0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use different learning rates ([0.01,0.1,0.5,1,2]) on your generated _AND_ and _XOR_ data and plot the accuracies against the epoch number, one plot each for _AND_ and _XOR_. Do bigger numbers necessarily lead to faster convergence or guarantee convergence at all? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = [0.01,0.1,0.5,1,2]\n",
    "pcts = [PerceptronArbitrary(np.random.randn(int(math.sqrt(len(data)))),np.random.randn()) for lr in lrs]\n",
    "pcts_xor = [PerceptronArbitrary(np.random.randn(int(math.sqrt(len(data)))),np.random.randn()) for lr in lrs]\n",
    "labels_xor = [1]+[0]*(len(data)-2)+[1]\n",
    "\n",
    "for i,lr in enumerate(lrs):\n",
    "    pcts[i].fit(data,labels,epochs=epochs,learning_rate=lr)\n",
    "    pcts_xor[i].fit(data,labels_xor,epochs=epochs,learning_rate=lr)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "for pct in pcts:\n",
    "    plt.plot(range(epochs),pct.acc)\n",
    "\n",
    "plt.title(\"Comparison of different learning rates to solve AND problem\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(lrs)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "for pct in pcts_xor:\n",
    "    plt.plot(range(epochs),pct.acc)\n",
    "\n",
    "plt.title(\"Comparison of different learning rates to solve XOR problem\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(lrs)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('airflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "568ce6a90abe48cd4c71813e2e18d608b5934a77a079188d46a97b4cb4032653"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
